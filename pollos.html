<!DOCTYPE html>
<html>

<head>
    <title>Información sobre PAC - Isaaquito</title>
    <link rel="stylesheet" href="clase1.css">
</head>

<body>
    <main>
        <h1>Análisis de componentes principales</h1>
        <nav>
            <ul>
                <li><a href="./isaac.html">Ir a Isaac página</a> <br>
                <li> <a href="http://www.google.com">Google</a> <br>
                <li> <a href="+51 993804361">Celular</a></li>
            </ul>
        </nav>
        <section>
            <p> En estadística, el <b>análisis de componentes principales </b>
                es una técnica utilizada para describir un conjunto de datos en términos de nuevas variables no
                correlacionadas.
            </p>
            <p>
                Técnicamente, el <i>ACP</i> busca la proyección según la cual los datos queden mejor representados en
                términos de mínimos cuadrados.
                Esta convierte un conjunto de observaciones de variables posiblemente correlacionadas en un conjunto de
                valores de variables
                sin correlación lineal llamadas componentes principales.
                <br>
                As with ANNs, many issues can arise with naively trained <a href="https://en.wikipedia.org/wiki/Deep_learning">DNNs</a>. Two common issues are overfitting and
                computation time.

                DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare
                dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning or
                weight decay or sparsity can be applied during training to combat overfitting. Alternatively
                dropout regularization randomly omits units from the hidden layers during training. This helps to
                exclude rare dependencies. <br> Finally, data can be augmented via methods such as cropping and rotating
                such that smaller training sets can be increased in size to reduce the chances of overfitting.

                DNNs must consider many training parameters, such as the size (number of layers and number of units per
                layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal
                parameters may not be feasible due to the cost in time and computational resources. Various tricks, such
                as batching (computing the gradient on several training examples at once rather than individual
                examples) speed up computation. Large processing capabilities of many-core architectures (such as
                GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability
                of such processing architectures for the matrix and vector computations.

                Alternatively, engineers may look for other types of neural networks with more straightforward and
                convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of
                neural network. It doesn't require learning rates or randomized initial weights for CMAC. The training
                process can be guaranteed to converge in one step with a new batch of data, and the computational
                complexity of the training algorithm is linear with respect to the number of neurons involved.
            </p>

            <!-- <img src="https://www.researchgate.net/profile/Gergely-Tholt/publication/325625808/figure/fig2/AS:634882522882050@1528379197963/Principal-component-analysis-PCA-Principal-component-analysis-PCA-biplot-of-EPG.png"
                alt="Gráfico de ResearchGate"> -->
            <hr>
        </section>
        <h2>Fundamento</h2>
        <p>El ACP construye una transformación lineal que escoge un nuevo sistema de coordenadas para el conjunto
            original de datos en el cual la
            varianza de mayor tamaño del conjunto de datos es capturada en el primer eje (llamado el Primer Componente
            Principal), la segunda varianza más grande es el segundo eje, y así sucesivamente. <br> Para construir esta
            transformación lineal debe
            construirse primero la matriz de covarianza o matriz de
            coeficientes de correlación.</p>
        <h2>Matemáticas del ACP</h2>
        <p>El ACP permite encontrar un número de factores subyacentes p < m que explican aproximadamente el valor de las
                m variables para cada individuo. El hecho de que existan estos p factores subyacentes puede
                interpretarse como una reducción de la dimensionalidad de los datos: donde antes necesitabamos m valores
                para caracterizar a cada individuo ahora nos bastan p valores. Cada uno de los p encontrados se llama
                componente principal, de ahí el nombre del método.</p>
                <h3>Método basado en correlaciones</h3>
                <ul id="miembros">
                    <li>Isaac Rojas</li>
                    <li>Karla Romero</li>
                    <li>Chisco
                        <ul>
                            <li>Francisco</li>
                            <li>Llaves</li>
                            <li>Jamonada</li>
                        </ul>
                    </li>
                    <li>xD ya no se que poner</li>
                </ul>
                <h3>Limitaciones</h3>
                <p>
                    Aqui un pequeño poema: <br> que sea lo mejor para el pais <br> voto por Vero para presidenta <br>
                    que
                    sea lo mejor para todos!
                </p>
                <div>C<sub>a</sub><sup>2</sup></div>
    </main>
    <footer>
        <hr>
        <p>Esta es mi página y el día de hoy es 12-04-2021</p>
    </footer>
</body>

</html>